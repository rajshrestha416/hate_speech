{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "120972c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras import backend as K\n",
    "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "9b6c29cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data\n",
    "# df_twitter = pd.read_csv(\"twitter_hate_speech.csv\")\n",
    "# df_nepali = pd.read_excel(\"nepali_hate_speech.xlsx\")\n",
    "\n",
    "df = pd.read_csv(\"merge_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "9ff09dd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "      <td>2</td>\n",
       "      <td>neither</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "      <td>1</td>\n",
       "      <td>offensive_language</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "      <td>1</td>\n",
       "      <td>offensive_language</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "      <td>1</td>\n",
       "      <td>offensive_language</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "      <td>1</td>\n",
       "      <td>offensive_language</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25855</th>\n",
       "      <td>hutihara</td>\n",
       "      <td>0</td>\n",
       "      <td>hate_speech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25856</th>\n",
       "      <td>hutihara</td>\n",
       "      <td>0</td>\n",
       "      <td>hate_speech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25857</th>\n",
       "      <td>haija</td>\n",
       "      <td>2</td>\n",
       "      <td>neither</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25858</th>\n",
       "      <td>hwau nabhayeko</td>\n",
       "      <td>0</td>\n",
       "      <td>hate_speech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25859</th>\n",
       "      <td>what the fuck</td>\n",
       "      <td>1</td>\n",
       "      <td>offensive_language</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25860 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label  \\\n",
       "0      !!! RT @mayasolovely: As a woman you shouldn't...      2   \n",
       "1      !!!!! RT @mleew17: boy dats cold...tyga dwn ba...      1   \n",
       "2      !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...      1   \n",
       "3      !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...      1   \n",
       "4      !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...      1   \n",
       "...                                                  ...    ...   \n",
       "25855                                           hutihara      0   \n",
       "25856                                           hutihara      0   \n",
       "25857                                              haija      2   \n",
       "25858                                     hwau nabhayeko      0   \n",
       "25859                                      what the fuck      1   \n",
       "\n",
       "                 category  \n",
       "0                 neither  \n",
       "1      offensive_language  \n",
       "2      offensive_language  \n",
       "3      offensive_language  \n",
       "4      offensive_language  \n",
       "...                   ...  \n",
       "25855         hate_speech  \n",
       "25856         hate_speech  \n",
       "25857             neither  \n",
       "25858         hate_speech  \n",
       "25859  offensive_language  \n",
       "\n",
       "[25860 rows x 3 columns]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "35f70492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_nepali"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a37267c",
   "metadata": {},
   "source": [
    "### Pre Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9dee7bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 25860 entries, 0 to 25859\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   text      25860 non-null  object\n",
      " 1   label     25860 non-null  int64 \n",
      " 2   category  25860 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 606.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "6b35f126",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "      <td>2</td>\n",
       "      <td>neither</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "      <td>1</td>\n",
       "      <td>offensive_language</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "      <td>1</td>\n",
       "      <td>offensive_language</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "      <td>1</td>\n",
       "      <td>offensive_language</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "      <td>1</td>\n",
       "      <td>offensive_language</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  \\\n",
       "0  !!! RT @mayasolovely: As a woman you shouldn't...      2   \n",
       "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...      1   \n",
       "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...      1   \n",
       "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...      1   \n",
       "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...      1   \n",
       "\n",
       "             category  \n",
       "0             neither  \n",
       "1  offensive_language  \n",
       "2  offensive_language  \n",
       "3  offensive_language  \n",
       "4  offensive_language  "
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "f53037bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Removing special characters, URLs, and extra whitespaces\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = re.sub(r'[^A-Za-z0-9]+', ' ', text)\n",
    "    # Tokenization\n",
    "    text = text.split()\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = [word for word in text if word not in stop_words]\n",
    "    \n",
    "    # Stemming\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    text = [stemmer.stem(word) for word in text]\n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "ff1dcb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "397661d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        rt mayasolov woman complain clean hous amp man...\n",
       "1        rt mleew17 boy dat cold tyga dwn bad cuffin da...\n",
       "2        rt urkindofbrand dawg rt 80sbaby4lif ever fuck...\n",
       "3               rt c g anderson viva base look like tranni\n",
       "4        rt shenikarobert shit hear might true might fa...\n",
       "                               ...                        \n",
       "25855                                             hutihara\n",
       "25856                                             hutihara\n",
       "25857                                                haija\n",
       "25858                                       hwau nabhayeko\n",
       "25859                                                 fuck\n",
       "Name: text, Length: 25860, dtype: object"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "bba2c800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoding\n",
    "# label_encoder = LabelEncoder()\n",
    "# df['label'] = label_encoder.fit_transform(data['class'])\n",
    "\n",
    "# Text Vectorization using TF-IDF\n",
    "# tfidf_vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2), stop_words='english')\n",
    "# X = tfidf_vectorizer.fit_transform(data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7eed57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "783903f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Selection using chi-squared test\n",
    "# k_best = SelectKBest(chi2, k=1000)\n",
    "# X = k_best.fit_transform(X, data['label'])\n",
    "# X = X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "a68b7da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['label'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "c48b8a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and validation sets\n",
    "# y = data['label']\n",
    "# Split the data into training, validation, and test sets\n",
    "# X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "cc1036ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18696    1\n",
       "341      2\n",
       "1510     1\n",
       "2201     1\n",
       "24739    1\n",
       "        ..\n",
       "5198     1\n",
       "2405     1\n",
       "17462    2\n",
       "21735    1\n",
       "24631    1\n",
       "Name: label, Length: 5172, dtype: int64"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "2b3de412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare the tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# tokenize the train and test dataset\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "29fd52e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Padding -> to uniform the datas\n",
    "max_length = max(len(seq) for seq in X_train)\n",
    "\n",
    "# to test an outlier case (if one of the test dataset has longer length)\n",
    "for x in X_test:\n",
    "    if len(x) > max_length:\n",
    "        print(f\"an outlier detected: {x}\")\n",
    "\n",
    "X_train = pad_sequences(X_train, maxlen = max_length)\n",
    "X_test = pad_sequences(X_test, maxlen = max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "6bda7dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = to_categorical(y_test, num_classes=3)\n",
    "y_train = to_categorical(y_train, num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "170982e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ..., 3378, 9744, 4836],\n",
       "       [   0,    0,    0, ...,  112, 2215,  323],\n",
       "       [   0,    0,    0, ...,  899, 1456, 9748],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,  345, 2212, 1362],\n",
       "       [   0,    0,    0, ...,  214,  307,   18],\n",
       "       [   0,    0,    0, ...,   27,  707,   91]], dtype=int32)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "110981bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num test tweet: 5172\n",
      "num train tweet: 20688\n"
     ]
    }
   ],
   "source": [
    "print(f\"num test tweet: {y_test.shape[0]}\")\n",
    "print(f\"num train tweet: {y_train.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "405f894c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    precisions = precision(y_true, y_pred)\n",
    "    recalls = recall(y_true, y_pred)\n",
    "    return 2*((precisions*recalls)/(precisions+recalls+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "80487b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change dis if u want\n",
    "output_dim = 200\n",
    "\n",
    "new_model = Sequential([\n",
    "    Embedding(vocab_size, output_dim, input_length=max_length),\n",
    "    # lstm for xxx\n",
    "    Bidirectional(LSTM(64, dropout=0.3, recurrent_dropout=0.3)),\n",
    "    # dropout to prevent overfitting\n",
    "    Dropout(0.5),\n",
    "    # dense to connect the previous output with current layer\n",
    "    Dense(128, activation=\"relu\"),\n",
    "    # dropout to prevent overfitting\n",
    "    Dropout(0.5),\n",
    "    # this is output layer, with 3 class (0, 1, 2)\n",
    "    Dense(3, activation=\"softmax\"),\n",
    "])\n",
    "\n",
    "new_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy',f1,precision, recall])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c36505ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 85, 200)           5063200   \n",
      "                                                                 \n",
      " bidirectional_2 (Bidirecti  (None, 128)               135680    \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 3)                 387       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5215779 (19.90 MB)\n",
      "Trainable params: 5215779 (19.90 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9303c6a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "324/324 [==============================] - 77s 233ms/step - loss: 0.4620 - accuracy: 0.8364 - f1: 0.8204 - precision: 0.8508 - recall: 0.7972 - val_loss: 0.3077 - val_accuracy: 0.8815 - val_f1: 0.8814 - val_precision: 0.8929 - val_recall: 0.8703\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model_history = new_model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size = 64,\n",
    "    epochs=1,\n",
    "    validation_data=(X_test, y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b68a805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63ba1558",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Padding -> to uniform the datas\n",
    "# max_length = max(seq.shape[0] for seq in X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b719604d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the BiLSTM Model\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(X_train.shape[0], output_dim=200, input_length=max_length))\n",
    "# model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "# model.add(Bidirectional(LSTM(64)))\n",
    "# model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a98c462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the Model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "266389f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks for optimization\n",
    "checkpoint = ModelCheckpoint(\"best_model.h5\", save_best_only=True, monitor='val_loss', mode='min')\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a51386a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "324/324 [==============================] - 899s 3s/step - loss: -3.6172 - accuracy: 0.7489 - val_loss: -6.4199 - val_accuracy: 0.7384 - lr: 0.0010\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "324/324 [==============================] - 817s 3s/step - loss: -8.1154 - accuracy: 0.7489 - val_loss: -11.0242 - val_accuracy: 0.7384 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "324/324 [==============================] - 771s 2s/step - loss: -12.3084 - accuracy: 0.7489 - val_loss: -15.5256 - val_accuracy: 0.7384 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "324/324 [==============================] - 814s 3s/step - loss: -16.5027 - accuracy: 0.7489 - val_loss: -20.0371 - val_accuracy: 0.7384 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "324/324 [==============================] - 817s 3s/step - loss: -20.6804 - accuracy: 0.7489 - val_loss: -24.5098 - val_accuracy: 0.7384 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "324/324 [==============================] - 814s 3s/step - loss: -24.8307 - accuracy: 0.7489 - val_loss: -28.9834 - val_accuracy: 0.7384 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "324/324 [==============================] - 838s 3s/step - loss: -28.9805 - accuracy: 0.7489 - val_loss: -33.4347 - val_accuracy: 0.7384 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "324/324 [==============================] - 1057s 3s/step - loss: -33.1282 - accuracy: 0.7489 - val_loss: -37.8951 - val_accuracy: 0.7384 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "324/324 [==============================] - 902s 3s/step - loss: -37.2668 - accuracy: 0.7489 - val_loss: -42.3658 - val_accuracy: 0.7384 - lr: 0.0010\n",
      "Epoch 10/10\n",
      "324/324 [==============================] - 1446s 4s/step - loss: -41.4149 - accuracy: 0.7489 - val_loss: -46.8246 - val_accuracy: 0.7384 - lr: 0.0010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2a0bff790>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Training\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=1, batch_size=64, callbacks=[checkpoint, early_stopping, reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a68ebf32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_model/assets\n"
     ]
    }
   ],
   "source": [
    "model.save('my_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a950e5ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9095bd77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - 15s 302ms/step\n"
     ]
    }
   ],
   "source": [
    "# Evaluation (using the X_test and y_test from previous split)\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_binary = (y_pred > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b335107c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [1],\n",
       "       ...,\n",
       "       [1],\n",
       "       [1],\n",
       "       [1]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc0a4b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5042e223",
   "metadata": {},
   "source": [
    "### Optimizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "dae7b0b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3y/yxfc9cms7ss262pddd2ymh_40000gp/T/ipykernel_28655/2610907151.py:1: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
      "  from kerastuner.tuners import RandomSearch\n"
     ]
    }
   ],
   "source": [
    "from kerastuner.tuners import RandomSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "bfd18f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hypermodel\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, hp.Int('embedding_dim', min_value=50, max_value=300, step=50), input_length=max_length))\n",
    "    model.add(Bidirectional(LSTM(hp.Int('lstm_units', min_value=32, max_value=128, step=32), dropout=hp.Float('lstm_dropout', min_value=0.2, max_value=0.5, step=0.1), recurrent_dropout=hp.Float('recurrent_dropout', min_value=0.2, max_value=0.5, step=0.1))))\n",
    "    model.add(Dropout(hp.Float('dropout_1', min_value=0.2, max_value=0.5, step=0.1)))\n",
    "    model.add(Dense(hp.Int('dense_units', min_value=64, max_value=256, step=32), activation=\"relu\"))\n",
    "    model.add(Dropout(hp.Float('dropout_2', min_value=0.2, max_value=0.5, step=0.1)))\n",
    "    model.add(Dense(3, activation=\"softmax\"))\n",
    "\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy', f1, precision, recall])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "61dfd162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the tuner\n",
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=5, \n",
    "    directory='my_dir',\n",
    "    project_name='hate_speech'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "9894768b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5 Complete [00h 01m 19s]\n",
      "val_accuracy: 0.8837973475456238\n",
      "\n",
      "Best val_accuracy So Far: 0.895591676235199\n",
      "Total elapsed time: 00h 14m 13s\n"
     ]
    }
   ],
   "source": [
    "# Search for the best hyperparameter configuration\n",
    "tuner.search(X_train, y_train, epochs=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1b32f670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best model\n",
    "best_model = tuner.get_best_models(num_models=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7ed61ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "647/647 [==============================] - 93s 141ms/step - loss: 0.2185 - accuracy: 0.9248 - f1: 0.9243 - precision: 0.9330 - recall: 0.9160 - val_loss: 0.3458 - val_accuracy: 0.8807 - val_f1: 0.8813 - val_precision: 0.8851 - val_recall: 0.8776\n"
     ]
    }
   ],
   "source": [
    "# Train the best model\n",
    "best_model_history = best_model.fit(X_train, y_train, epochs=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c691f67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5fd8ef3b",
   "metadata": {},
   "source": [
    "###  GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d322f4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5 Complete [00h 02m 47s]\n",
      "val_accuracy: 0.8934648036956787\n",
      "\n",
      "Best val_accuracy So Far: 0.8934648036956787\n",
      "Total elapsed time: 00h 07m 27s\n",
      "647/647 [==============================] - 176s 269ms/step - loss: 0.2269 - accuracy: 0.9210 - f1: 0.9210 - precision: 0.9284 - recall: 0.9140 - val_loss: 0.3166 - val_accuracy: 0.8853 - val_f1: 0.8853 - val_precision: 0.8899 - val_recall: 0.8809\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import GRU\n",
    "\n",
    "# Define the hypermodel for GRU\n",
    "def build_gru_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, hp.Int('embedding_dim', min_value=50, max_value=300, step=50), input_length=max_length))\n",
    "    model.add(Bidirectional(GRU(hp.Int('gru_units', min_value=32, max_value=128, step=32), dropout=hp.Float('gru_dropout', min_value=0.2, max_value=0.5, step=0.1), recurrent_dropout=hp.Float('recurrent_dropout', min_value=0.2, max_value=0.5, step=0.1))))\n",
    "    model.add(Dropout(hp.Float('dropout_1', min_value=0.2, max_value=0.5, step=0.1)))\n",
    "    model.add(Dense(hp.Int('dense_units', min_value=64, max_value=256, step=32), activation=\"relu\"))\n",
    "    model.add(Dropout(hp.Float('dropout_2', min_value=0.2, max_value=0.5, step=0.1)))\n",
    "    model.add(Dense(3, activation=\"softmax\"))\n",
    "\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy', f1, precision, recall])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Instantiate the tuner for GRU\n",
    "gru_tuner = RandomSearch(\n",
    "    build_gru_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=5,\n",
    "    directory='my_gru_dir',\n",
    "    project_name='hate_speech_gru'\n",
    ")\n",
    "\n",
    "# Search for the best hyperparameter configuration for GRU\n",
    "gru_tuner.search(X_train, y_train, epochs=1, validation_data=(X_test, y_test))\n",
    "\n",
    "# Get the best GRU model\n",
    "best_gru_model = gru_tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "# Train the best GRU model\n",
    "best_gru_model_history = best_gru_model.fit(X_train, y_train, epochs=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1139847",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "f1da33de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_category(text, model, max_length):\n",
    "    # Preprocess the input text\n",
    "    preprocessed_text = preprocess_text(text)\n",
    "\n",
    "    # Tokenize and pad the sequence\n",
    "    # declare the tokenizer\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(text)\n",
    "\n",
    "    # tokenize the train and test dataset\n",
    "    X_input = tokenizer.texts_to_sequences(text)\n",
    "    X_input = pad_sequences(X_input, maxlen=max_length)\n",
    "\n",
    "    # Make predictions using the trained model\n",
    "    prediction = model.predict(X_input)[0]\n",
    "\n",
    "    #Convert the prediction to a category label\n",
    "    predicted_label = np.argmax(prediction)\n",
    "\n",
    "    # Map the category label to its original class name\n",
    "#     predicted_category = category_mapping[predicted_label]\n",
    "\n",
    "    return predicted_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "6aca79a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 25ms/step\n",
      "Predicted Category: 1\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "input_text = \"Black people not allowed\"\n",
    "predicted_category = predict_category(input_text, best_model, max_length)\n",
    "\n",
    "print(\"Predicted Category:\", predicted_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de52e931",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88923e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
